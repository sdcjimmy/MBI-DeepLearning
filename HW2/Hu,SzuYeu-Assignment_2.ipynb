{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please include your name below\n",
    "__Student name:__ Szu-Yeu, Hu\n",
    "\n",
    "### Please cite the reference(s) you looked up and the name of students you collaborated with\n",
    "__Reference(s) and collaborator(s):__\n",
    "Ref: https://en.wikipedia.org/wiki/Chest_radiograph\n",
    "Collaborator: Chen, Dong\n",
    "\n",
    "\n",
    "### Data and Preliminaries\n",
    "In this assignment, we will explore the NIH CXR8 database (https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community). This chest X-ray database is the basis of a series highly-cited papers/preprint papers since 2017. You will examine the images in this dataset and build your own convolutional neural networks to classify the major diagnoses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Data summary (10 points)\n",
    "Deep learning is not immune from the 'garbage in, garbage out' principle. Before digging into the data, it is recommended to get a sense of how the data was generated, understand the assumptions of the data, and review the data quality. We will ask you to answer some basic questions on the NIH CXR8 dataset. Please visit the website of the NIH CXR8 database, download the metadata (Data_Entry_2017.csv) and answer the following questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.1 (5 points)\n",
    "What is the file format of images in the NIH CXR8 database? What is the standard format for radiology image storage and transmission? How many images are there in the database? How many diagnostic categories are there in the database? What are they? How many images have more than one diagnosis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Answer:__\n",
    "\n",
    "- In NIH CXR8 database, the images are in PNG format in 1024 by 1024 resolution. \n",
    "\n",
    "- The standard format for radiology storage adn transmission is in DICOM(Digital Imaging and Communications in Medicine )format.\n",
    "\n",
    "- There are totally 112,120 images in the database. \n",
    "\n",
    "- There are 14 diagnostic categories and 1 category for \"No Finding\"\n",
    "\n",
    "- The 14 diagnostic categories are Atelectasis, Consolidation, Infiltration, Pneumothorax, Edema, Emphysema, Fibrosis, Effusion, Pneumonia, Pleural_thickening, Cardiomegaly, Nodule, Mass and Hernia\n",
    "\n",
    "- There are 20,796 images have more than one diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2 (5 points)\n",
    "How many patients are there in total? How many patients contributed more than one image? Which patient ID contributed the most images and how many did he or she contribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Answer:__\n",
    "\n",
    "- There are totally 30,805 patients in the database.\n",
    "\n",
    "- Of all the patients, there are 13,302 of them contributed to more than one image\n",
    "\n",
    "- Patient ID 10007 contribute the most images in the database. This patients has 187 images totally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Check the images (10 points)\n",
    "In the following questions, you will be asked to examine the images in the NIH CXR8 dataset. The images for Question 2.1 and 2.2 could be found at https://www.dropbox.com/sh/2h068ge9xv1g27u/AAAXVq8VYXF6HRlHvzvjy-e6a?dl=0. Feel free to collaborate with other students or consult any references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1 (2 points)\n",
    "What is the NIH-labeled diagnosis of image `00001583_014.png`? What did you see in this image? Word limit: 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Answer:__\n",
    "- The diagnosis of the image is pneumothorax. \n",
    "- The image is in PA view. There are right side diaphgram elevation, indicating volumn loss of the right lung. A tube is seen at the right side, could be chest tube or central venous catheter. There is no obvious lung margin seen in this image, the pneumothorax might be treated already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2 (3 points)\n",
    "What is the NIH-labeled diagnosis of image `00000019_000.png`? What did you see in this image? Word limit: 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Answer:__\n",
    "\n",
    "- The diagnosis of the image is Atelectasis|Effusion|Pleural_Thickening.\n",
    "- The image in in PA view. There is a medical devices at the right side of the patient. The right side diaphgram is elevated, suggesting right lung collapse or atelectasis. Left costovertebral angle is mildly blunt, indicating pleural effusion at left side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3 (5 points)\n",
    "What is \"View Position?\" How does it affect the resulting chest X-ray images visually? Word limit: 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Answer:__\n",
    "\n",
    "View position is the relative orientation of the body and the direction of the x-ray beam. The most common views are posteroanterior, anteroposterior, and lateral. In the images above, it's the standard PA view. The X-reay beam enters through the posterior aspect of the chest, and exits out of the anterior aspect where the beam is detected. \n",
    "\n",
    "Differnt view will affect the relative size of heart and lung. The heart, being an anterior structure within the chest, will look larger in AP view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Build a custom convolutional neural network (25 points)\n",
    "For this question, we ask you to build a multi-layer convolutional neural network to classify a subset of cardiomegaly images from normal ones. Please download the training set and the validation set here (https://www.dropbox.com/sh/ojiw79q8786ua4x/AAAtaJVKEdv91Zybpi-fAfMsa?dl=0). Please DO NOT use any other image from NIH CXR8 or from other databases for this question. Feel free to use keras or any other high-level deep learning packages to classify the images.\n",
    "\n",
    "Design a convolutional neural network with at least two convolution layers, at least one max-pooling layer, and at least one dropout layer. Although you should explore various combinations of hyperparameters, we will grade this question based on the accuracy of the implementation, not the performance of the network.\n",
    "\n",
    "What is your design? What binary loss/accuracy did you get in the training and validation set? Please include your code in the assignment submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "## Read in the labels\n",
    "train_labels = pd.read_csv('train.csv?dl=1',  header=None, index_col=0)\n",
    "val_labels = pd.read_csv('val.csv?dl=1',  header=None, index_col=0)\n",
    "\n",
    "## We will resize the images to make them smaller\n",
    "image_size = (224,224)\n",
    "\n",
    "## Read in the training images\n",
    "train_images = []\n",
    "train_dir = './train/'\n",
    "train_files = os.listdir(train_dir)\n",
    "for f in train_files:\n",
    "    img = Image.open(train_dir + f)\n",
    "    img = img.resize(image_size)\n",
    "    img_arr = np.array(img)\n",
    "    train_images.append(img_arr)\n",
    "\n",
    "train_X = np.array(train_images)\n",
    "\n",
    "## Read in the val images\n",
    "val_images = []\n",
    "val_dir = './val/'\n",
    "val_files = os.listdir(val_dir)\n",
    "for f in val_files:\n",
    "    img = Image.open(val_dir + f)\n",
    "    img = img.resize(image_size)\n",
    "    img_arr = np.array(img)\n",
    "    val_images.append(img_arr)\n",
    "\n",
    "train_X = np.array(train_images)\n",
    "val_X = np.array(val_images)\n",
    "\n",
    "# Labels processing\n",
    "train_labels = train_labels.reindex(train_files)\n",
    "val_labels = val_labels.reindex(val_files)\n",
    "\n",
    "label_transformer = LabelBinarizer()\n",
    "train_y = label_transformer.fit_transform(train_labels)\n",
    "val_y = label_transformer.transform(val_labels)\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "train_X = train_X.astype(np.float32)\n",
    "val_X = val_X.astype(np.float32)\n",
    "\n",
    "train_X /= 255.\n",
    "val_X /= 255.\n",
    "\n",
    "train_X = np.expand_dims(train_X, 3)\n",
    "val_X = np.expand_dims(val_X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, adam\n",
    "\n",
    "image_size = (96,96)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3),strides=(1,1), padding=\"same\",input_shape = (image_size[0],image_size[1],1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), strides=(1,1), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3),strides=(1,1), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1,1), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer= 'adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X, train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(val_X, val_y),\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 1750 samples, validate on 437 samples\n",
    "Epoch 1/20\n",
    " - 8s - loss: 0.6961 - acc: 0.5211 - val_loss: 0.6935 - val_acc: 0.5011\n",
    "Epoch 2/20\n",
    " - 7s - loss: 0.6866 - acc: 0.5514 - val_loss: 0.6740 - val_acc: 0.6041\n",
    "Epoch 3/20\n",
    " - 7s - loss: 0.6342 - acc: 0.6383 - val_loss: 0.6546 - val_acc: 0.6201\n",
    "Epoch 4/20\n",
    " - 7s - loss: 0.6135 - acc: 0.6857 - val_loss: 0.6371 - val_acc: 0.6568\n",
    "Epoch 5/20\n",
    " - 7s - loss: 0.5799 - acc: 0.7034 - val_loss: 0.6172 - val_acc: 0.6499\n",
    "Epoch 6/20\n",
    " - 7s - loss: 0.5433 - acc: 0.7269 - val_loss: 0.6410 - val_acc: 0.6636\n",
    "Epoch 7/20\n",
    " - 7s - loss: 0.5374 - acc: 0.7406 - val_loss: 0.5746 - val_acc: 0.6911\n",
    "Epoch 8/20\n",
    " - 7s - loss: 0.5235 - acc: 0.7451 - val_loss: 0.6056 - val_acc: 0.6705\n",
    "Epoch 9/20\n",
    " - 7s - loss: 0.4727 - acc: 0.7731 - val_loss: 0.5955 - val_acc: 0.7071\n",
    "Epoch 10/20\n",
    " - 7s - loss: 0.4176 - acc: 0.8109 - val_loss: 0.7464 - val_acc: 0.7071\n",
    "Epoch 11/20\n",
    " - 7s - loss: 0.3606 - acc: 0.8383 - val_loss: 0.6861 - val_acc: 0.7025\n",
    "Epoch 12/20\n",
    " - 7s - loss: 0.3212 - acc: 0.8669 - val_loss: 0.7526 - val_acc: 0.7002\n",
    "Epoch 13/20\n",
    " - 7s - loss: 0.2378 - acc: 0.9006 - val_loss: 0.9940 - val_acc: 0.6545\n",
    "Epoch 14/20\n",
    " - 7s - loss: 0.1805 - acc: 0.9269 - val_loss: 1.1154 - val_acc: 0.6842\n",
    "Epoch 15/20\n",
    " - 7s - loss: 0.1567 - acc: 0.9366 - val_loss: 1.0583 - val_acc: 0.6636\n",
    "Epoch 16/20\n",
    " - 7s - loss: 0.1329 - acc: 0.9497 - val_loss: 1.1874 - val_acc: 0.6659\n",
    "Epoch 17/20\n",
    " - 7s - loss: 0.0802 - acc: 0.9731 - val_loss: 1.3704 - val_acc: 0.7048\n",
    "Epoch 18/20\n",
    " - 7s - loss: 0.0684 - acc: 0.9754 - val_loss: 1.8591 - val_acc: 0.6568\n",
    "Epoch 19/20\n",
    " - 7s - loss: 0.0765 - acc: 0.9697 - val_loss: 1.5689 - val_acc: 0.6842\n",
    "Epoch 20/20\n",
    " - 7s - loss: 0.0585 - acc: 0.9817 - val_loss: 1.6600 - val_acc: 0.6659\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 96, 96, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 48, 48, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               4718720   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,783,841\n",
      "Trainable params: 4,783,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Answer:__<br>\n",
    "Model summary: _see the above cell_<br>\n",
    "Binary loss in the training set: 0.0585<br>\n",
    "Accuracy of the training set: 0.9817<br>\n",
    "Binary loss in the validation set: __best__: 0.5746, __last__: 1.6600<br>\n",
    "Accuracy of the validation set: __best__:0.7048, __last__:0.6659<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Transfer learning: Using the VGGNet (16 layers) architecture (20 points)\n",
    "For this question, we ask you to employ VGGNet, a convolutional neural network built for ImageNet, to classify the same subset of cardiomegaly images from normal ones (https://www.dropbox.com/sh/ojiw79q8786ua4x/AAAtaJVKEdv91Zybpi-fAfMsa?dl=0). We encourage you to take a look at the documentation for keras.applications (https://keras.io/applications/) and reuse their modules. Please DO NOT use any other images from NIH CXR8 or from other databases for this question. Although you should explore various combinations of hyperparameters, we will grade this question based on the accuracy of the implementation, not the performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1 (10 points)\n",
    "What is your best validation accuracy of fine-tuning a 16-layer VGGNet WITHOUT ImageNet weights? In your model with the lowest validation loss, what are the hyperparameters? What is the validation loss? What is the training loss/accuracy? Please include your code in the assignment submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from skimage.color import gray2rgb\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "## Read in the labels\n",
    "train_labels = pd.read_csv('train.csv?dl=1',  header=None, index_col=0)\n",
    "val_labels = pd.read_csv('val.csv?dl=1',  header=None, index_col=0)\n",
    "\n",
    "## We will resize the images to make them smaller\n",
    "image_size = (224,224)\n",
    "\n",
    "## Read in the training images\n",
    "train_images = []\n",
    "train_dir = './train/'\n",
    "train_files = os.listdir(train_dir)\n",
    "for f in train_files:\n",
    "    img = Image.open(train_dir + f)\n",
    "    img = img.resize(image_size)\n",
    "    img_arr = np.array(img)\n",
    "    train_images.append(img_arr)\n",
    "\n",
    "train_X = np.array(train_images)\n",
    "\n",
    "## Read in the val images\n",
    "val_images = []\n",
    "val_dir = './val/'\n",
    "val_files = os.listdir(val_dir)\n",
    "for f in val_files:\n",
    "    img = Image.open(val_dir + f)\n",
    "    img = img.resize(image_size)\n",
    "    img_arr = np.array(img)\n",
    "    val_images.append(img_arr)\n",
    "\n",
    "train_X = np.array(train_images)\n",
    "val_X = np.array(val_images)\n",
    "\n",
    "\n",
    "## Now we are going to reorder the labels just to make sure they line up with \n",
    "## with the order of the files which we just read in. Always better to be sure!\n",
    "train_labels = train_labels.reindex(train_files)\n",
    "val_labels = val_labels.reindex(val_files)\n",
    "label_transformer = LabelBinarizer()\n",
    "train_y = label_transformer.fit_transform(train_labels)\n",
    "val_y = label_transformer.transform(val_labels)\n",
    "\n",
    "\n",
    "train_X = gray2rgb(train_X)\n",
    "val_X = gray2rgb(val_X)\n",
    "\n",
    "train_X = train_X.astype(np.float32)\n",
    "val_X = val_X.astype(np.float32)\n",
    "\n",
    "train_X = preprocess_input(train_X)\n",
    "val_X = preprocess_input(val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights=None, include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "x = base_model.output\n",
    "# add a 2D global average pooling layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# add a layer for binary classification\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# define the model to be trained\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 1750 samples, validate on 437 samples\n",
    "Epoch 1/10\n",
    " - 25s - loss: 0.6955 - acc: 0.4903 - val_loss: 0.6942 - val_acc: 0.4943\n",
    "Epoch 2/10\n",
    " - 21s - loss: 0.6950 - acc: 0.4771 - val_loss: 0.6932 - val_acc: 0.4897\n",
    "Epoch 3/10\n",
    " - 21s - loss: 0.6943 - acc: 0.4920 - val_loss: 0.6938 - val_acc: 0.5011\n",
    "Epoch 4/10\n",
    " - 21s - loss: 0.6951 - acc: 0.5131 - val_loss: 0.6930 - val_acc: 0.5011\n",
    "Epoch 5/10\n",
    " - 21s - loss: 0.6921 - acc: 0.5217 - val_loss: 0.7016 - val_acc: 0.4989\n",
    "Epoch 6/10\n",
    " - 21s - loss: 0.6951 - acc: 0.5051 - val_loss: 0.6926 - val_acc: 0.5080\n",
    "Epoch 7/10\n",
    " - 21s - loss: 0.6939 - acc: 0.5006 - val_loss: 0.6929 - val_acc: 0.5011\n",
    "Epoch 8/10\n",
    " - 21s - loss: 0.6932 - acc: 0.4914 - val_loss: 0.6926 - val_acc: 0.5011\n",
    "Epoch 9/10\n",
    " - 21s - loss: 0.6928 - acc: 0.4983 - val_loss: 0.6923 - val_acc: 0.5561\n",
    "Epoch 10/10\n",
    " - 21s - loss: 0.6920 - acc: 0.5137 - val_loss: 0.6940 - val_acc: 0.4989\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Answer:__<br>\n",
    "The loss function you used: binary_crossentropy<br>\n",
    "Binary loss in the training set: 0.6920<br>\n",
    "Accuracy of the training set: __best__: 0.5217  __last__:0.5137<br>\n",
    "Binary loss in the validation set: 0.6923<br>\n",
    "Accuracy of the validation set: __best:__0.5561, __last__:0.4989<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2 (10 points)\n",
    "What is your best validation accuracy of fine-tuning a 16-layer VGGNet WITH ImageNet weights? In your model with the lowest validation loss, what are the hyperparameters? What is the validation loss? What is the training loss/accuracy? Please include your code in the assignment submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "x = base_model.output\n",
    "# add a 2D global average pooling layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# add a layer for binary classification\n",
    "x = Dense(128, activation = 'relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# define the model to be trained\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "batch_size = 32\n",
    "model.fit(train_X, train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(val_X, val_y),\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on 1750 samples, validate on 437 samples\n",
    "Epoch 1/10\n",
    " - 21s - loss: 1.1292 - acc: 0.5303 - val_loss: 0.9514 - val_acc: 0.5538\n",
    "Epoch 2/10\n",
    " - 21s - loss: 0.8349 - acc: 0.5891 - val_loss: 0.8223 - val_acc: 0.6087\n",
    "Epoch 3/10\n",
    " - 21s - loss: 0.7409 - acc: 0.6251 - val_loss: 0.7683 - val_acc: 0.6156\n",
    "Epoch 4/10\n",
    " - 21s - loss: 0.6756 - acc: 0.6703 - val_loss: 0.7318 - val_acc: 0.6156\n",
    "Epoch 5/10\n",
    " - 21s - loss: 0.6481 - acc: 0.6766 - val_loss: 0.7117 - val_acc: 0.6384\n",
    "Epoch 6/10\n",
    " - 21s - loss: 0.6027 - acc: 0.7103 - val_loss: 0.6859 - val_acc: 0.6682\n",
    "Epoch 7/10\n",
    " - 21s - loss: 0.5741 - acc: 0.7194 - val_loss: 0.6741 - val_acc: 0.6636\n",
    "Epoch 8/10\n",
    " - 21s - loss: 0.5599 - acc: 0.7257 - val_loss: 0.6559 - val_acc: 0.6888\n",
    "Epoch 9/10\n",
    " - 21s - loss: 0.5393 - acc: 0.7451 - val_loss: 0.6467 - val_acc: 0.6773\n",
    "Epoch 10/10\n",
    " - 21s - loss: 0.5328 - acc: 0.7463 - val_loss: 0.6630 - val_acc: 0.6728\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Answer:__<br>\n",
    "The loss function you used: binary_crossentrophy<br>\n",
    "Binary loss in the training set: 0.5328<br>\n",
    "Accuracy of the training set: 0.7463<br>\n",
    "Binary loss in the validation set:0.6630<br>\n",
    "Accuracy of the validation set: 0.6728<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Multiclass classification and the BMI707 Kaggle contest (30 points)\n",
    "In this question, we will build multiclass classifiers to distinguish different types of lung diseases using the NIHCXR8 data.\n",
    "Please download the training set and the validation set from the BMI707 Kaggle contest website (https://www.kaggle.com/c/bmi707-assignment-2-q5/data ). Please note that this dataset is different from the one we used in Question 3 and 4. Please DO NOT use any additional dataset (including those from NIH CXR8) to train or augment your models. Feel free to use any (ImageNet or any custom) architecture to classify all available classes. In your model with the lowest validation loss, what are the hyperparameters? What is the validation loss/accuracy? What is the training loss/accuracy? Please participate in the BMI707 internal Kaggle contest (https://www.kaggle.com/c/bmi707-assignment-2-q5) and compare your results with others there. An ensemble of models is allowed. The top 5 submissions with the highest accuracy on the private test set (testPrivate.tar) will receive bonus points. Please include your code in the assignment submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from skimage.color import gray2rgb\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras import regularizers\n",
    "\n",
    "# Data Loading ...\n",
    "train_labels = pd.read_csv('train.csv',  header=None, index_col=0)\n",
    "\n",
    "image_size = (224,224)\n",
    "## Read in the training images\n",
    "train_images = []\n",
    "train_dir = './train/'\n",
    "train_files = os.listdir(train_dir)\n",
    "for f in train_files:\n",
    "    img = Image.open(train_dir + f)\n",
    "    img = img.resize(image_size)\n",
    "    img_arr = np.array(img)\n",
    "    train_images.append(img_arr)\n",
    "\n",
    "train_X = np.array(train_images)\n",
    "\n",
    "train_labels = train_labels.reindex(train_files)\n",
    "\n",
    "label_transformer = LabelBinarizer()\n",
    "train_y = label_transformer.fit_transform(train_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Data Preprocessing ...\n",
    "train_X = train_X.astype(np.float32)\n",
    "#train_X /= 255.\n",
    "train_X = gray2rgb(train_X)\n",
    "\n",
    "\n",
    "# Data Augmentation\n",
    "if augmentation:\n",
    "    print(\"Data Augmentation...\")\n",
    "\n",
    "    for i in range(10):\n",
    "        new_X = load_hdf5(\"augment_data/batch\"+str(i)+\"_aug_X.h5\")\n",
    "        new_y = load_hdf5(\"augment_data/batch\"+str(i)+\"_aug_y.h5\")\n",
    "\n",
    "        train_X = np.concatenate((train_X, new_X), axis = 0)\n",
    "        train_y = np.concatenate((train_y, new_y), axis = 0)\n",
    "\n",
    "    print(\"Finish augmentation\")\n",
    "    print(\"X training Shape: \" + str(train_X.shape))\n",
    "    print(\"y training Shape: \" + str(train_y.shape))\n",
    "\n",
    "train_X = preprocess_input(train_X)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape = train_X.shape[1:])\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation = 'relu')(x)\n",
    "\n",
    "predictions = Dense(15, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "# define the model to be trained\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "print(model.summary())\n",
    "\n",
    "if trainablelayer == 0:\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint( 'result/' + experiment + '_best_weights.h5', verbose=1, monitor='val_loss', mode='auto', save_best_only=True)\n",
    "\n",
    "batch_size = 32\n",
    "model.fit(train_X, train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_split = 0.2,\n",
    "          callbacks=[checkpointer]\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = []\n",
    "test_dir = './val/'\n",
    "test_files = os.listdir(test_dir)\n",
    "for f in test_files:\n",
    "    img = Image.open(test_dir + f)\n",
    "    img = img.resize(image_size)\n",
    "    img_arr = np.array(img)\n",
    "    test_images.append(img_arr)\n",
    "\n",
    "test_X = np.array(test_images)\n",
    "\n",
    "# Data Preprocessing ...\n",
    "test_X = test_X.astype(np.float32)\n",
    "#test_X /= 255.\n",
    "test_X = gray2rgb(test_X)\n",
    "test_X = preprocess_input(test_X)\n",
    "\n",
    "prediction = model.predict(test_X)\n",
    "\n",
    "write_hdf5(prediction,\"val_prediction.h5\")\n",
    "\n",
    "sub = {\"Id\": os.listdir(test_dir), \"Category\":prediction.argmax(1)}\n",
    "val_submission = pd.DataFrame(sub, columns=sub.keys())\n",
    "\n",
    "\n",
    "test_images = []\n",
    "test_dir = './test/'\n",
    "test_files = os.listdir(test_dir)\n",
    "for f in test_files:\n",
    "    img = Image.open(test_dir + f)\n",
    "    img = img.resize(image_size)\n",
    "    img_arr = np.array(img)\n",
    "    test_images.append(img_arr)\n",
    "\n",
    "test_X = np.array(test_images)\n",
    "\n",
    "# Data Preprocessing ...\n",
    "test_X = test_X.astype(np.float32)\n",
    "#test_X /= 255.\n",
    "test_X = gray2rgb(test_X)\n",
    "test_X = preprocess_input(test_X)\n",
    "\n",
    "prediction = model.predict(test_X)\n",
    "\n",
    "write_hdf5(prediction,\"val_prediction.h5\")\n",
    "\n",
    "sub = {\"Id\": os.listdir(test_dir), \"Category\":prediction.argmax(1)}\n",
    "test_submission = pd.DataFrame(sub, columns=sub.keys())\n",
    "\n",
    "submission = pd.concat([val_submission,test_submission])\n",
    "submission = submission.sort_values(\"Id\")\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 15)                1935      \n",
      "=================================================================\n",
      "Total params: 14,880,847\n",
      "Trainable params: 14,880,847\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Answer:__<br>\n",
    "Summary of model(s): VGG16 architecture with global average pooling and 3 dense layer. _(Detail model shown as above cell)_<br>\n",
    "The loss function you used: categorical_crossentrophy<br>\n",
    "Loss in the training set: 1.0089<br>\n",
    "Accuracy of the training set: 0.7119<br>\n",
    "Loss in the validation set: 1.1024<br>\n",
    "Accuracy of the validation set: 0.6951<br>\n",
    "__Remember to participate in the BMI707 internal Kaggle contest (https://www.kaggle.com/c/bmi707-assignment-2-q5)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Limitations of the NIH CXR8 dataset and the road ahead (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.1 (3 points)\n",
    "Please list three limitations of models trained from this dataset. Word limit: 150 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Answer:__\n",
    "\n",
    "- 1. The labels are imbalanced. More than 60% of the labels are the 11th class, which is \"no finding\" in the dataset. Therefore the models tend to predict all the outcome to be this class. Even if the model can achieve about 70% of accuracy, it still perform bad on detecting abnormal findings. \n",
    "\n",
    "- 2. The model is a blackbox, so it's hard to know how the model dectect disease. For example, we cannot not tell the model actually detect pneumothorax from the vanished lung marking, or just simply from the presence of chest tube.\n",
    "\n",
    "- 3. There are only 15 classes in this dataset. But in real life, there should be more different conditions. The model trained from this dataset can not tell the disease that are not presented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.2 (2 points)\n",
    "What are the potential roadblocks to implementing automated chest X-ray film reader in the clinical settings? Word limit: 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Answer:__\n",
    "In clinical setting, we need large amount of well-labeled data to train the model. However, in real-life the diagnosis might be different between different radiologists. Depending on the data source we collected, we could accidentally include the bias of the radiologist itself. Even we can reach high accuracy, it would still be hard to reach consensus between the radiologists and an artificial intelligence.\n",
    "\n",
    "Also, it is difficult to obtain the images of rare but potentially fatal diseases like cancer, and would make it harder for the model to recognize the condition. However, it's crucial for an automated CXR reader to detect such disease so that we won't miss the important timing of treatment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
